{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/minyoung347/highfrequency.git\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/highfrequency')"
      ],
      "metadata": {
        "id": "_POpMndXIkGG",
        "outputId": "33d74287-bb6a-420e-c924-af200f662cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'highfrequency'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 46 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), 6.39 MiB | 1.26 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_nnf8-qGIKyN",
        "outputId": "af3d945d-4d86-48f0-adcc-e9164bcbbdab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-11 15:50:46,750 - VOC_TOPICS - INFO - Using computation device: cpu\n"
          ]
        }
      ],
      "source": [
        "import typing\n",
        "from typing import Tuple\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import utils\n",
        "from modules import Encoder, Decoder\n",
        "from custom_types import DaRnnNet, TrainData, TrainConfig\n",
        "from utils import numpy_to_tensor\n",
        "from constants import device\n",
        "\n",
        "logger = utils.setup_log()\n",
        "logger.info(f\"Using computation device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "YaOnon5tJ0Ov",
        "outputId": "e52eab19-551a-4ebf-ea4a-a1cb203ebb5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TCY7JMDIKyO"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(dat, col_names, scale=None) -> Tuple[TrainData, StandardScaler]:\n",
        "    if scale is None:\n",
        "        scale = StandardScaler().fit(dat)\n",
        "    proc_dat = scale.transform(dat)\n",
        "    proc_dat = np.array(proc_dat)  # 2023\n",
        "    # proc_dat = np.array(dat)  # 2023\n",
        "    mask = np.ones(proc_dat.shape[1], dtype=bool)\n",
        "    dat_cols = list(dat.columns)\n",
        "    for col_name in col_names:\n",
        "        mask[dat_cols.index(col_name)] = False\n",
        "\n",
        "    feats = proc_dat[:, mask]\n",
        "    targs = proc_dat[:, ~mask]\n",
        "\n",
        "    return TrainData(feats, targs), scale\n",
        "\n",
        "\n",
        "def da_rnn(train_data: TrainData, n_targs: int, encoder_hidden_size=50, decoder_hidden_size=50, # 50\n",
        "           T=10, learning_rate=0.001, batch_size=128):\n",
        "\n",
        "    train_cfg = TrainConfig(T, int(train_data.feats.shape[0] * 0.7), batch_size, nn.MSELoss())\n",
        "    logger.info(f\"Training size: {train_cfg.train_size:d}.\")\n",
        "\n",
        "    enc_kwargs = {\"input_size\": train_data.feats.shape[1], \"hidden_size\": encoder_hidden_size, \"T\": T}\n",
        "    encoder = Encoder(**enc_kwargs).to(device)\n",
        "    with open(os.path.join(\"data\", \"enc_kwargs.json\"), \"w\") as fi:\n",
        "        json.dump(enc_kwargs, fi, indent=4)\n",
        "\n",
        "    dec_kwargs = {\"encoder_hidden_size\": encoder_hidden_size,\n",
        "                  \"decoder_hidden_size\": decoder_hidden_size, \"T\": T, \"out_feats\": n_targs}\n",
        "    decoder = Decoder(**dec_kwargs).to(device)\n",
        "    with open(os.path.join(\"data\", \"dec_kwargs.json\"), \"w\") as fi:\n",
        "        json.dump(dec_kwargs, fi, indent=4)\n",
        "\n",
        "    encoder_optimizer = optim.Adam(\n",
        "        params=[p for p in encoder.parameters() if p.requires_grad],\n",
        "        lr=learning_rate, weight_decay = 1e-6) # weight_decay = 1e-5\n",
        "    decoder_optimizer = optim.Adam(\n",
        "        params=[p for p in decoder.parameters() if p.requires_grad],\n",
        "        lr=learning_rate, weight_decay = 1e-6)\n",
        "    da_rnn_net = DaRnnNet(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "\n",
        "    return train_cfg, da_rnn_net\n",
        "\n",
        "\n",
        "def train(net: DaRnnNet, train_data: TrainData, t_cfg: TrainConfig, n_epochs=10, save_plots=True):\n",
        "    iter_per_epoch = int(np.ceil(t_cfg.train_size * 1. / t_cfg.batch_size))\n",
        "    iter_losses = np.zeros(n_epochs * iter_per_epoch)\n",
        "    epoch_losses = np.zeros(n_epochs)\n",
        "    iter_losses_val = np.zeros(n_epochs * iter_per_epoch) #\n",
        "    epoch_losses_val = np.zeros(n_epochs) #\n",
        "    logger.info(f\"Iterations per epoch: {t_cfg.train_size * 1. / t_cfg.batch_size:3.3f} ~ {iter_per_epoch:d}.\")\n",
        "\n",
        "    n_iter = 0\n",
        "\n",
        "    for e_i in range(n_epochs):\n",
        "        perm_idx = np.random.permutation(t_cfg.train_size - t_cfg.T)\n",
        "\n",
        "        t_iter = 0\n",
        "        for t_i in range(0, t_cfg.train_size, t_cfg.batch_size):\n",
        "            batch_idx = perm_idx[t_i : t_i + t_cfg.batch_size]\n",
        "            feats, y_history, y_target = prep_train_data(batch_idx, t_cfg, train_data)\n",
        "\n",
        "            loss, attn_weight = train_iteration(net, t_cfg.loss_func, feats, y_history, y_target)\n",
        "            if t_iter == 0:\n",
        "                torch.save(attn_weight, './pt_save/attn_w_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.pt'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size, e_i, t_iter))\n",
        "                torch.save(y_target, './pt_save/y_target_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.pt'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size, e_i, t_iter))\n",
        "                y_test_pred = predict(net, train_data, #\n",
        "                                  t_cfg.train_size, t_cfg.batch_size, t_cfg.T, #\n",
        "                                  on_train=False) #\n",
        "                torch.save(y_test_pred, './pt_save/y_pred_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.pt'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size, e_i, t_iter))\n",
        "            \n",
        "            y_test_pred = predict(net, train_data, #\n",
        "                                  t_cfg.train_size, t_cfg.batch_size, t_cfg.T, #\n",
        "                                  on_train=False) #\n",
        "            # TODO: make this MSE and make it work for multiple inputs\n",
        "            val_loss = y_test_pred - train_data.targs[t_cfg.train_size:] #\n",
        "            iter_losses_val[e_i * iter_per_epoch + t_i // t_cfg.batch_size] = np.mean(np.abs(val_loss)) #\n",
        "            \n",
        "            \n",
        "            iter_losses[e_i * iter_per_epoch + t_i // t_cfg.batch_size] = loss\n",
        "            # if (j / t_cfg.batch_size) % 50 == 0:\n",
        "            #    self.logger.info(\"Epoch %d, Batch %d: loss = %3.3f.\", i, j / t_cfg.batch_size, loss)\n",
        "            \n",
        "            file_iter_loss = open('./loss_save/iter_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size),'a')\n",
        "            file_iter_loss.write('{}, {}, {}, {}\\n'.format(e_i, n_iter, loss, np.mean(np.abs(val_loss))))\n",
        "            file_iter_loss.close()\n",
        "            \n",
        "            n_iter += 1\n",
        "\n",
        "            adjust_learning_rate(net, n_iter)\n",
        "            \n",
        "            t_iter += 1\n",
        "\n",
        "        epoch_losses[e_i] = np.mean(iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])\n",
        "        epoch_losses_val[e_i] = np.mean(iter_losses_val[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])\n",
        "        \n",
        "        file_epoch_loss = open('./loss_save/epoch_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size),'a')\n",
        "        file_epoch_loss.write('{}, {}, {}, {}\\n'.format(e_i, n_iter, np.mean(iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)]), np.mean(np.abs(val_loss)), np.mean(iter_losses_val[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])))\n",
        "        file_epoch_loss.close()\n",
        "        \n",
        "\n",
        "        if e_i % 10 == 0:\n",
        "            y_test_pred = predict(net, train_data,\n",
        "                                  t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
        "                                  on_train=False)\n",
        "            # TODO: make this MSE and make it work for multiple inputs\n",
        "            val_loss = y_test_pred - train_data.targs[t_cfg.train_size:]\n",
        "            #logger.info(f\"Epoch {e_i:d}, niter {n_iter:d}, train loss: {epoch_losses[e_i]:3.3f}, val loss: {np.mean(np.abs(val_loss))}.\")\n",
        "            logger.info(f\"Epoch {e_i:d}, niter {n_iter:d}, train loss: {epoch_losses[e_i]}, val loss: {np.mean(np.abs(val_loss))}.\")\n",
        "            \n",
        "            \n",
        "            y_train_pred = predict(net, train_data,\n",
        "                                   t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
        "                                   on_train=True)\n",
        "\n",
        "            plt.figure(figsize=(10,7))\n",
        "            #plt.subplot(1,3,1)\n",
        "            plt.plot(range(1, 1 + len(train_data.targs)), train_data.targs,\n",
        "                     'r', label=\"True\")\n",
        "            plt.plot(range(t_cfg.T, len(y_train_pred) + t_cfg.T), y_train_pred,\n",
        "                     'g' ,label='Predicted - Train')\n",
        "            plt.plot(range(t_cfg.T + len(y_train_pred), len(train_data.targs) + 1), y_test_pred,\n",
        "                     'b', label='Predicted - Test')\n",
        "            plt.legend(loc='upper left')\n",
        "            #plt.show()\n",
        "            #plt.subplot(1,3,2)\n",
        "            #plt.scatter(train_data.targs[:len(y_train_pred)], y_train_pred)\n",
        "            #plt.subplot(1,3,3)\n",
        "            #plt.scatter(train_data.targs[len(y_train_pred):], y_test_pred)\n",
        "            utils.save_or_show_plot(\"pred_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.png\".format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size, e_i), save_plots)\n",
        "            plt.close()\n",
        "            \n",
        "            '''\n",
        "            correct_case = 0\n",
        "            uncorrect_case = 0\n",
        "            for i, ii in zip(train_data.targs[:len(y_train_pred)], y_train_pred):\n",
        "                if i != 0:\n",
        "                    if np.sign(i) == np.sign(ii):\n",
        "                        correct_case += 1\n",
        "                    else:\n",
        "                        uncorrect_case += 1\n",
        "            acc_train = correct_case / (correct_case + uncorrect_case)\n",
        "            \n",
        "            correct_case = 0\n",
        "            uncorrect_case = 0\n",
        "            for i, ii in zip(train_data.targs[len(y_train_pred):], y_test_pred):\n",
        "                if i != 0:\n",
        "                    if np.sign(i) == np.sign(ii):\n",
        "                        correct_case += 1\n",
        "                    else:\n",
        "                        uncorrect_case += 1\n",
        "            acc_test = correct_case / (correct_case + uncorrect_case)\n",
        "            \n",
        "            file = open('./pred_save/acc_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs),'a')\n",
        "            file.write('{}, {}\\n'.format(e_i, n_iter, acc_train, acc_test))\n",
        "            file.close()\n",
        "            '''\n",
        "           \n",
        "    return iter_losses, epoch_losses, iter_losses_val, epoch_losses_val\n",
        "\n",
        "\n",
        "def prep_train_data(batch_idx: np.ndarray, t_cfg: TrainConfig, train_data: TrainData):\n",
        "    feats = np.zeros((len(batch_idx), t_cfg.T - 1, train_data.feats.shape[1]))\n",
        "    y_history = np.zeros((len(batch_idx), t_cfg.T - 1, train_data.targs.shape[1]))\n",
        "    y_target = train_data.targs[batch_idx + t_cfg.T]\n",
        "\n",
        "    for b_i, b_idx in enumerate(batch_idx):\n",
        "        start, stop = b_idx, b_idx + t_cfg.T - 1\n",
        "        feats[b_i, :, :] = train_data.feats[start : stop, :]\n",
        "        y_history[b_i, :] = train_data.targs[start : stop]\n",
        "\n",
        "    return feats, y_history, y_target\n",
        "\n",
        "\n",
        "def adjust_learning_rate(net: DaRnnNet, n_iter: int):\n",
        "    # TODO: Where did this Learning Rate adjustment schedule come from?\n",
        "    # Should be modified to use Cosine Annealing with warm restarts https://www.jeremyjordan.me/nn-learning-rate/\n",
        "    #if n_iter % 10000 == 0 and n_iter > 0:\n",
        "    if n_iter % 1000 == 0 and n_iter > 0:\n",
        "        for enc_params, dec_params in zip(net.enc_opt.param_groups, net.dec_opt.param_groups):\n",
        "            enc_params['lr'] = enc_params['lr'] * 0.9\n",
        "            dec_params['lr'] = dec_params['lr'] * 0.9\n",
        "\n",
        "\n",
        "def train_iteration(t_net: DaRnnNet, loss_func: typing.Callable, X, y_history, y_target):\n",
        "    t_net.enc_opt.zero_grad()\n",
        "    t_net.dec_opt.zero_grad()\n",
        "\n",
        "    input_weighted, input_encoded, attn_w = t_net.encoder(numpy_to_tensor(X))\n",
        "    y_pred = t_net.decoder(input_encoded, numpy_to_tensor(y_history))\n",
        "\n",
        "    y_true = numpy_to_tensor(y_target)\n",
        "    loss = loss_func(y_pred, y_true)\n",
        "    loss.backward()\n",
        "\n",
        "    t_net.enc_opt.step()\n",
        "    t_net.dec_opt.step()\n",
        "\n",
        "    return loss.item(), attn_w\n",
        "\n",
        "\n",
        "def predict(t_net: DaRnnNet, t_dat: TrainData, train_size: int, batch_size: int, T: int, on_train=False):\n",
        "    out_size = t_dat.targs.shape[1]\n",
        "    if on_train:\n",
        "        y_pred = np.zeros((train_size - T + 1, out_size))\n",
        "    else:\n",
        "        y_pred = np.zeros((t_dat.feats.shape[0] - train_size, out_size))\n",
        "\n",
        "    for y_i in range(0, len(y_pred), batch_size):\n",
        "        y_slc = slice(y_i, y_i + batch_size)\n",
        "        batch_idx = range(len(y_pred))[y_slc]\n",
        "        b_len = len(batch_idx)\n",
        "        X = np.zeros((b_len, T - 1, t_dat.feats.shape[1]))\n",
        "        y_history = np.zeros((b_len, T - 1, t_dat.targs.shape[1]))\n",
        "\n",
        "        for b_i, b_idx in enumerate(batch_idx):\n",
        "            if on_train:\n",
        "                start, stop = b_idx, b_idx + T - 1\n",
        "            else:\n",
        "                start, stop = b_idx + train_size - T, b_idx + train_size - 1\n",
        "\n",
        "            X[b_i, :, :] = t_dat.feats[start : stop, :]\n",
        "            y_history[b_i, :] = t_dat.targs[start : stop]\n",
        "\n",
        "        y_history = numpy_to_tensor(y_history)\n",
        "        _, input_encoded, _ = t_net.encoder(numpy_to_tensor(X))\n",
        "        y_pred[y_slc] = t_net.decoder(input_encoded, y_history).cpu().data.numpy()\n",
        "\n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYnXg35ZIKyQ"
      },
      "outputs": [],
      "source": [
        "save_plots = True\n",
        "debug = False\n",
        "    \n",
        "firm_num = 1\n",
        "start_day = 1\n",
        "end_day = 169\n",
        "add_random = 0\n",
        "# num_variables = 81 # exclude target variable\n",
        "num_variables = 90 # exclude target variable\n",
        "lr = 0.01\n",
        "num_epoch = 100\n",
        "bs = 1024\n",
        "tc = 10\n",
        "ehs = 100\n",
        "dhs = 100\n",
        "agg_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIdp6kwSIKyQ",
        "outputId": "281b723d-cc4c-4f40-db15-e29bebf10937"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-11 23:37:56,724 - VOC_TOPICS - INFO - Shape of data: (14349, 91).\n",
            "Missing in data: 0.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if os.path.exists('./loss_save/iter_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size)):\n",
        "    os.remove('./loss_save/iter_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size))\n",
        "if os.path.exists('./loss_save/epoch_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size)):\n",
        "    os.remove('./loss_save/epoch_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size))\n",
        "if os.path.exists('./pred_save/final_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size)):\n",
        "    os.remove('./pred_save/final_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size))\n",
        "\n",
        "\n",
        "if start_day == end_day:\n",
        "    if add_random == 0:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}.csv\".format(firm_num, start_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 1:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_r.csv\".format(firm_num, start_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 2:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_re.csv\".format(firm_num, start_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 3:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_re_r.csv\".format(firm_num, start_day, agg_size)), nrows=100 if debug else None) \n",
        "    elif add_random == 4:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_all.csv\".format(firm_num, start_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 5:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_all_r.csv\".format(firm_num, start_day, agg_size)), nrows=100 if debug else None)    \n",
        "else:\n",
        "    if add_random == 0:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_{}.csv\".format(firm_num, start_day, end_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 1:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_{}_r.csv\".format(firm_num, start_day, end_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 2:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_{}_re.csv\".format(firm_num, start_day, end_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 3:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_{}_re_r.csv\".format(firm_num, start_day, end_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 4:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_{}_all.csv\".format(firm_num, start_day, end_day, agg_size)), nrows=100 if debug else None)\n",
        "    elif add_random == 5:\n",
        "        raw_data = pd.read_csv(os.path.join(\"data/LSE_orderbook\", \"da_f{}_{}_{}_{}_all_r.csv\".format(firm_num, start_day, end_day, agg_size)), nrows=100 if debug else None)\n",
        "\n",
        "\n",
        "# raw_data = pd.read_csv('nasdaq100_padding.csv')\n",
        "# raw_data.columns = names=['var' + str(i) for i in range(1,83)]\n",
        "# raw_data = raw_data.iloc[0:2000]\n",
        "    \n",
        "    \n",
        "logger.info(f\"Shape of data: {raw_data.shape}.\\nMissing in data: {raw_data.isnull().sum().sum()}.\")\n",
        "if add_random == 1 or add_random == 3 or add_random == 5:\n",
        "    targ_cols = (\"var{}\".format(2*num_variables + 1),)\n",
        "else:\n",
        "    targ_cols = (\"var{}\".format(num_variables + 1),)\n",
        "data, scaler = preprocess_data(raw_data, targ_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmE-KZRsIKyQ",
        "outputId": "88ffb9ea-31f4-4f1b-bb77-7016adbf7d49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-11 23:37:59,938 - VOC_TOPICS - INFO - Training size: 10044.\n"
          ]
        }
      ],
      "source": [
        "da_rnn_kwargs = {\"batch_size\": bs, \"T\": tc, \"encoder_hidden_size\": ehs, \"decoder_hidden_size\": dhs} # 1024, 10, 50, 50\n",
        "config, model = da_rnn(data, n_targs=len(targ_cols), learning_rate=lr, **da_rnn_kwargs) #.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqGRnFORIKyQ",
        "outputId": "6b699d55-ecaa-440a-bcaf-672bd44298a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-11 23:38:02,352 - VOC_TOPICS - INFO - Iterations per epoch: 9.809 ~ 10.\n",
            "2023-06-11 23:38:04,766 - VOC_TOPICS - INFO - Epoch 0, niter 10, train loss: 0.8997158288955689, val loss: 0.8396776787280622.\n",
            "2023-06-11 23:38:28,107 - VOC_TOPICS - INFO - Epoch 10, niter 110, train loss: 0.8721830427646637, val loss: 0.8359963524105062.\n",
            "2023-06-11 23:38:49,742 - VOC_TOPICS - INFO - Epoch 20, niter 210, train loss: 0.8641083598136902, val loss: 0.8353187410807841.\n",
            "2023-06-11 23:39:11,251 - VOC_TOPICS - INFO - Epoch 30, niter 310, train loss: 0.8331318795681, val loss: 0.8472448199834722.\n",
            "2023-06-11 23:39:32,812 - VOC_TOPICS - INFO - Epoch 40, niter 410, train loss: 0.804358696937561, val loss: 0.8628435953636514.\n",
            "2023-06-11 23:39:54,386 - VOC_TOPICS - INFO - Epoch 50, niter 510, train loss: 0.7447859704494476, val loss: 0.8823428216192687.\n",
            "2023-06-11 23:40:15,992 - VOC_TOPICS - INFO - Epoch 60, niter 610, train loss: 0.6742965817451477, val loss: 0.9044516389674948.\n",
            "2023-06-11 23:40:38,585 - VOC_TOPICS - INFO - Epoch 70, niter 710, train loss: 0.8768493354320526, val loss: 0.8417217509155823.\n",
            "2023-06-11 23:41:00,203 - VOC_TOPICS - INFO - Epoch 80, niter 810, train loss: 0.8699835658073425, val loss: 0.8447401293128606.\n",
            "2023-06-11 23:41:21,789 - VOC_TOPICS - INFO - Epoch 90, niter 910, train loss: 0.8351188778877259, val loss: 0.8522619253591793.\n"
          ]
        }
      ],
      "source": [
        "iter_loss, epoch_loss, iter_loss_val, epoch_loss_val = train(model, data, config, n_epochs=num_epoch, save_plots=save_plots)\n",
        "final_y_pred = predict(model, data, config.train_size, config.batch_size, config.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbajHuDTIKyR",
        "outputId": "4da16855-63b4-431d-c9dc-ee9e09cd01b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.675493612078978\n"
          ]
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.semilogy(range(len(iter_loss)), iter_loss, label='train loss')\n",
        "plt.semilogy(range(len(iter_loss_val)), iter_loss_val, label='valiation loss')\n",
        "utils.save_or_show_plot(\"iter_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.png\".format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size), save_plots)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.semilogy(range(len(epoch_loss)), epoch_loss, label='epoch train loss')\n",
        "plt.semilogy(range(len(epoch_loss_val)), epoch_loss_val, label='epoch valiadation loss')\n",
        "utils.save_or_show_plot(\"epoch_loss_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.png\".format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size), save_plots)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(final_y_pred, label='Predicted')\n",
        "plt.plot(data.targs[config.train_size:], label=\"True\")\n",
        "plt.legend(loc='upper left')\n",
        "utils.save_or_show_plot(\"final_predicted_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.png\".format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size), save_plots)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "correct_case = 0\n",
        "uncorrect_case = 0\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "for i, ii in zip(data.targs[config.train_size:], final_y_pred):\n",
        "    if i[0] != 0:\n",
        "        if np.sign(i[0]) == np.sign(ii[0]):\n",
        "            correct_case += 1\n",
        "        else:\n",
        "            uncorrect_case += 1\n",
        "            \n",
        "        if np.sign(i[0]) > 0:\n",
        "            if np.sign(ii[0]) > 0:\n",
        "                TP += 1\n",
        "            elif np.sign(ii[0]) < 0:\n",
        "                FN += 1\n",
        "        elif np.sign(i[0]) < 0: \n",
        "            if np.sign(ii[0]) > 0:\n",
        "                FP += 1\n",
        "            elif np.sign(ii[0]) < 0:\n",
        "                TN += 1\n",
        "                            \n",
        "acc_final = correct_case / (correct_case + uncorrect_case)\n",
        "print(acc_final)\n",
        "\n",
        "\n",
        "file_stat = open('./stat_save/stat_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size),'a')\n",
        "file_stat.write('{}, {}, {}, {}, {}\\n'.format(acc_final, TP, FN, FP, TN))\n",
        "file_stat.close()\n",
        "\n",
        "\n",
        "\n",
        "file_iter_loss = open('./pred_save/final_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.csv'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size),'a')\n",
        "for i, ii in zip(data.targs[config.train_size:], final_y_pred):\n",
        "    file_iter_loss.write('{}, {}\\n'.format(i[0], ii[0]))\n",
        "file_iter_loss.close()\n",
        "\n",
        "with open(os.path.join(\"data\", \"da_rnn_kwargs.json\"), \"w\") as fi:\n",
        "    json.dump(da_rnn_kwargs, fi, indent=4)\n",
        "\n",
        "joblib.dump(scaler, os.path.join(\"data\", \"scaler.pkl\"))\n",
        "torch.save(model.encoder.state_dict(), os.path.join(\"data\", \"encoder.torch\"))\n",
        "torch.save(model.decoder.state_dict(), os.path.join(\"data\", \"decoder.torch\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N4znTKmIKyR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "firm_num = 1\n",
        "start_day = 1\n",
        "end_day = 169\n",
        "add_random = 0\n",
        "# num_variables = 81 # exclude target variable\n",
        "num_variables = 90 # exclude target variable\n",
        "lr = 0.01\n",
        "num_epoch = 10\n",
        "bs = 1024\n",
        "tc = 10\n",
        "ehs = 100\n",
        "dhs = 100\n",
        "agg_size = 100\n",
        "\n",
        "e_i = 1\n",
        "t_iter = 0\n",
        "\n",
        "attn_w = torch.load('./pt_save/attn_w_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.pt'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size, e_i, t_iter))\n",
        "attn_w = attn_w.detach().cpu().numpy()\n",
        "\n",
        "y_target = torch.load('./pt_save/y_target_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}.pt'.format(firm_num, start_day, end_day, add_random, num_variables, lr, num_epoch, bs, tc, ehs, dhs, agg_size, e_i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYJRJ_vLIKyR",
        "outputId": "811a697d-ef40-4ebe-b9d7-e0c626ed46e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1024, 9, 90)\n",
            "(1024, 1)\n"
          ]
        }
      ],
      "source": [
        "print(np.shape(attn_w))\n",
        "print(np.shape(y_target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igKgfHJIIKyR",
        "outputId": "6f286ba6-ccef-4dc4-f6ee-8df0f8315d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1024, 9, 90)\n",
            "(1024, 1)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (81,) (90,) (81,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[73], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y_target)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m y_target[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m y_target[i] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m         attn_positive \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m np\u001b[39m.\u001b[39;49mmean(attn_w[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m], axis \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m     \u001b[39melif\u001b[39;00m y_target[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m y_target[i] \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m         attn_negative \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(attn_w[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (81,) (90,) (81,) "
          ]
        }
      ],
      "source": [
        "# attn_positive = np.zeros(62*2)\n",
        "# attn_negative = np.zeros(62*2)\n",
        "attn_positive = np.zeros(num_variables)\n",
        "attn_negative = np.zeros(num_variables)\n",
        "\n",
        "print(np.shape(attn_w))\n",
        "print(np.shape(y_target))\n",
        "\n",
        "for i in range(len(y_target)-1):\n",
        "    if y_target[i+1] - y_target[i] > 0:\n",
        "        attn_positive += np.mean(attn_w[i+1], axis = 0)\n",
        "    elif y_target[i+1] - y_target[i] < 0:\n",
        "        attn_negative += np.mean(attn_w[i+1], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsD3Y1GJIKyR"
      },
      "outputs": [],
      "source": [
        "plt.plot(attn_positive)\n",
        "plt.savefig('test.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}